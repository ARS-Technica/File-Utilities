"""
A simple script to download every available issue of a single manga series hosted on https://w15.mangafreak.net. 
"""


import os
import requests
from bs4 import BeautifulSoup
import re
from tqdm import tqdm
import tkinter as tk
from tkinter import ttk
import urllib.parse
import time

def get_manga_title(url):
    manga_title = url.split('/')[-1]
    manga_title = manga_title.replace("_", " ")
    return manga_title

def get_save_folder():
    folder_path = input("Enter the folder path where you want to save the manga files: ")
    folder_path = folder_path.strip()

    if not os.path.isdir(folder_path):
        print("Invalid folder path. Please provide a valid existing folder path.")
        return get_save_folder()

    return folder_path

def download_files(url, folder_path, progress_bar, session):
    response = session.get(url)
    soup = BeautifulSoup(response.text, 'html.parser')
    all_links = soup.find_all('a')

    # Filter links containing the word "Download"
    download_links = [link for link in all_links if 'Download' in link.text]

    for link in download_links:
        file_name = link.text.strip()
        file_name = file_name.replace("_", " ")

        file_url = link.get('href')
        if not file_url:
            continue

        # Construct the full URL if it's a relative URL
        if not file_url.startswith(('http:', 'https:')):
            file_url = urllib.parse.urljoin(url, file_url)

        with open(os.path.join(folder_path, file_name), 'wb') as file:
            file_response = session.get(file_url, stream=True)

            if file_response.status_code != 200:
                print(f"Failed to download {file_name}")
                continue

            total_size = int(file_response.headers.get('content-length', 0))
            block_size = 1024

            with tqdm(total=total_size, unit='B', unit_scale=True, desc=file_name) as pbar:
                for data in file_response.iter_content(block_size):
                    pbar.update(len(data))
                    file.write(data)
        progress_bar.step(1)

def main():
    url = input("Enter the URL: ")

    if not url.startswith("https://w15.mangafreak.net"):
        print("Invalid URL. Please provide a valid MangaFreak URL.")
        return

    manga_title = get_manga_title(url)

    folder_path = get_save_folder()

    manga_folder = os.path.join(folder_path, manga_title.replace(" ", "_"))
    os.makedirs(manga_folder, exist_ok=True)

    root = tk.Tk()
    root.title("Manga Scraper Progress")

    overall_progress = tk.DoubleVar()
    overall_progress.set(0)

    overall_progress_bar = ttk.Progressbar(root, variable=overall_progress, maximum=100)
    overall_progress_bar.pack(pady=10)

    # Create a session to maintain cookies between requests
    session = requests.Session()

    # Set a user-agent header to mimic a web browser
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36'
    }
    session.headers.update(headers)

    # Retry in case of CloudFlare challenges (adjust as needed)
    max_retries = 5
    retries = 0

    while retries < max_retries:
        try:
            download_files(url, manga_folder, overall_progress_bar, session)
            root.destroy()
            print("Manga Downloaded Successfully!")
            break
        except requests.exceptions.RequestException as e:
            # Handle CloudFlare challenges or other network issues
            print(f"Error: {e}")
            print("Retrying...")
            retries += 1
            time.sleep(5)  # Wait before retrying (adjust as needed)

    if retries >= max_retries:
        print("Max retries reached. Failed to download manga.")

if __name__ == "__main__":
    main()

